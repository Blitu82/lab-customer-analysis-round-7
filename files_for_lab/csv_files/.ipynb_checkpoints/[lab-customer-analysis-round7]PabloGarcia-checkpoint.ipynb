{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f71e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14de905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this lab, we still keep using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder.\n",
    "\n",
    "# It's time to put it all together. Remember the previous rounds and follow the steps as shown in previous lectures.\n",
    "\n",
    "# 01 - Problem (case study)\n",
    "    # Data Description.\n",
    "    # Goal.\n",
    "    \n",
    "# 02 - Getting Data\n",
    "    # Read the .csv file.\n",
    "    \n",
    "# 03 - Cleaning/Wrangling/EDA\n",
    "    # Change headers names.\n",
    "    # Deal with NaN values.\n",
    "    # Categorical Features.\n",
    "    # Numerical Features.\n",
    "    # Exploration.\n",
    "    \n",
    "# 04 - Processing Data\n",
    "    # Dealing with outliers.\n",
    "    # Normalization.\n",
    "    # Encoding Categorical Data.\n",
    "    # Splitting into train set and test set.\n",
    "    \n",
    "# 05 - Modeling\n",
    "    # Apply model.\n",
    "    \n",
    "# 06 - Model Validation\n",
    "    # R2.\n",
    "    # MSE.\n",
    "    # RMSE.\n",
    "    # MAE.\n",
    "    \n",
    "# 07 - Reporting\n",
    "    # Present results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337851ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants that will be used during processing:\n",
    "\n",
    "LAB2_CSV_FILE = 'marketing_customer_analysis.csv'\n",
    "NEW_COLUMNS = ['customer', 'state', 'customer_lifetime_value',\n",
    "       'response', 'coverage', 'education', 'effective_to_date',\n",
    "       'employment_status', 'gender', 'income', 'location_code',\n",
    "       'marital_status', 'monthly_premium_auto', 'months_since_last_claim',\n",
    "       'months_since_policy_inception', 'number_of_open_complaints',\n",
    "       'number_of_policies', 'policy_type', 'policy', 'renew_offer_type',\n",
    "       'sales_channel', 'total_claim_amount', 'vehicle_class', 'vehicle_size',\n",
    "       'vehicle_type']\n",
    "PERCENTAGE_NAN = 50\n",
    "Y_VAR = 'total_claim_amount'\n",
    "THRESHOLD = 3\n",
    "COLS_TO_LOG = ['customer_lifetime_value', 'income', 'monthly_premium_auto']\n",
    "COLS_TO_TRANSFORM = ['customer_lifetime_value', 'income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception']\n",
    "TEST_SIZE = 0.4\n",
    "RANDOM_STATE = 100\n",
    "CATEGORICAL_CSV_FILE = 'categorical_from_lab4.csv'\n",
    "NUMERICAL_CSV_FILE = 'df_numeric_x_lab5.csv'\n",
    "Y_CSV_FILE = 'df_y_lab5.csv'\n",
    "ONE_HOT_COLS = ['customer', 'state', 'response', 'coverage', 'education',\n",
    "       'effective_to_date', 'employment_status', 'gender', 'location_code',\n",
    "       'marital_status', 'policy_type', 'policy', 'renew_offer_type',\n",
    "       'sales_channel', 'vehicle_class', 'vehicle_size']\n",
    "LABEL_ENCODER_COLS = ['customer', 'state', 'response', 'coverage', 'education',\n",
    "       'effective_to_date', 'employment_status', 'gender', 'location_code',\n",
    "       'marital_status', 'policy_type', 'policy', 'renew_offer_type',\n",
    "       'sales_channel', 'vehicle_class', 'vehicle_size']\n",
    "# TEST_SIZE = 0.4\n",
    "# RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9df39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that will be used during processing\n",
    "\n",
    "\n",
    "# 02 - Getting Data\n",
    "    # Read the .csv file.\n",
    "\n",
    "def import_to_df(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "    return df\n",
    "\n",
    "# 03 - Cleaning/Wrangling/EDA\n",
    "    # Change headers names.\n",
    "    \n",
    "def rename_columns(df, new_columns):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.columns = new_columns\n",
    "    return df_copy   \n",
    "    \n",
    "    # Deal with NaN values.\n",
    "    \n",
    "def check_nan_values(df):\n",
    "    null_df = df.isna().sum()/len(df)*100\n",
    "    print('The percentage of NaN values per column is:\\n',null_df)\n",
    "    \n",
    "def remove_nan_cols(df, percentage_nan):\n",
    "    df_copy = df.copy()\n",
    "    null_df = df.isna().sum()/len(df)*100\n",
    "    for item in null_df.index:\n",
    "        if null_df.loc[item] > percentage_nan:\n",
    "            df_copy.drop([item], inplace=True, axis=1)\n",
    "            print(\"Dropping column:\", item, \"due to percentage of NaN values greater than:\", percentage_nan) \n",
    "    return df_copy  \n",
    "\n",
    "def remove_nan_rows(df):\n",
    "    df_copy = df.copy()\n",
    "    return df_copy.dropna()\n",
    "\n",
    "# 03 - Cleaning/Wrangling/EDA\n",
    "    # Outliers???\n",
    "# def filter_outliers(df, threshold):\n",
    "#     mask = pd.Series(data=True, index=df.index)\n",
    "#     for col in df.columns:\n",
    "#         q1 = df[col].quantile(0.25)\n",
    "#         q3 = df[col].quantile(0.75)\n",
    "#         iqr = q3 - q1\n",
    "#         lower_limit = q1 - (threshold * iqr)\n",
    "#         upper_limit = q3 + (threshold * iqr)\n",
    "#         col_mask = (df[col] >= lower_limit) & (df[col] <= upper_limit)\n",
    "#         mask = mask & col_mask\n",
    "#     return df[mask]\n",
    "    \n",
    "    # Categorical Features. Numerical Features.\n",
    "    \n",
    "def to_object(df, col1, col2):\n",
    "    df[col1] = df[col1].astype(str)\n",
    "    df[col2] = df[col2].astype(str)\n",
    "    return df   \n",
    "    \n",
    "def split_categorical_numerical(df):\n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "    numerical_df = df.select_dtypes(exclude=['object'])\n",
    "    return categorical_df, numerical_df\n",
    "    \n",
    "    # Exploration.\n",
    "        \n",
    "# 04 - Processing Data\n",
    "    # Dealing with outliers.\n",
    "    # Normalization.\n",
    "    \n",
    "def split_numerical(numerical_df, y_var):\n",
    "    df_copy = numerical_df.copy()\n",
    "    x = df_copy.drop(y_var, axis=1)\n",
    "    y = df_copy[y_var]\n",
    "    return x,y\n",
    "\n",
    "def plot_hist(df):\n",
    "    num_cols = len(df.columns)\n",
    "    num_subplots = num_cols // 2 + num_cols % 2\n",
    "    fig, axis = plt.subplots(2,num_subplots, figsize=(12,8))  \n",
    "    axis = axis.ravel()\n",
    "    for i, col in enumerate(df.columns):\n",
    "        sns.histplot(df[col], kde=True, ax = axis[i]).set_xlabel(col)     \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_box(df):\n",
    "    num_cols = len(df.columns)\n",
    "    num_subplots = num_cols // 2 + num_cols % 2\n",
    "    fig, axis = plt.subplots(2,num_subplots, figsize=(12,8))  \n",
    "    axis = axis.ravel()\n",
    "    for i, col in enumerate(df.columns):\n",
    "        sns.boxplot(x = df[col], ax = axis[i]).set_xlabel(col)     \n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "def replace_inf(i):\n",
    "    if np.isfinite(i):\n",
    "        return i\n",
    "    else:\n",
    "        return np.NAN    \n",
    "    \n",
    "def log_transformer(df, cols_to_log):\n",
    "    df_copy = df.copy()\n",
    "    for col in cols_to_log:\n",
    "        df_copy[col] = np.log(df_copy[col])\n",
    "        df_copy[col] = list(map(replace_inf, df_copy[col]))\n",
    "        df_copy[col] = df_copy[col].fillna(df_copy[col].mean())\n",
    "    return df_copy\n",
    "\n",
    "def box_cox_transformer(df, cols_to_transform):\n",
    "    df_copy = df.copy()\n",
    "    for col in cols_to_transform:\n",
    "        df_copy[col] = np.where(df_copy[col] <= 0, 0, df_copy[col])\n",
    "        df_copy[col].replace(0,df[col].mean(), inplace=True)\n",
    "        transformed_col, _ = stats.boxcox(df_copy[col])\n",
    "        df_copy[col] = transformed_col\n",
    "    return df_copy\n",
    "\n",
    "def min_max_scaler(df):\n",
    "    df_copy = pd.DataFrame(MinMaxScaler().fit(df).transform(df))\n",
    "    return df_copy\n",
    "\n",
    "def standard_scaler(df):\n",
    "    columns = df.columns\n",
    "    df_copy = pd.DataFrame(StandardScaler().fit(df).transform(df))\n",
    "    df_copy.columns = columns\n",
    "    return df_copy\n",
    "\n",
    "    # Encoding Categorical Data.\n",
    "    # Splitting into train set and test set.    \n",
    "    \n",
    "\n",
    "def label_encoder(df, label_encoder_cols):\n",
    "    df_copy = df.copy()\n",
    "    for col in label_encoder_cols:\n",
    "        df_copy[col] = LabelEncoder().fit(df_copy[col]).transform(df_copy[col])\n",
    "    return df_copy\n",
    "\n",
    "def one_hot_encoder(df, one_hot_cols):\n",
    "    df_encoded = df.copy()\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(df_encoded[one_hot_cols].values)\n",
    "    encoded = encoder.transform(df_encoded[one_hot_cols].values)\n",
    "    encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names_out(one_hot_cols))\n",
    "    df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "    df_encoded = df_encoded.drop(one_hot_cols, axis=1)\n",
    "    return df_encoded       \n",
    "        \n",
    "def concat_df(df1, df2):\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df_combined = pd.concat([df1, df2], axis=1, ignore_index=True)\n",
    "    df_combined = sm.add_constant(df_combined)\n",
    "    return df_combined\n",
    "\n",
    "# def train_test_split(X, y, test_size=0.4, random_state=50):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,random_state=random_state)\n",
    "#     return X_train, y_train, X_test, y_test\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    X = sm.add_constant(X) # Add a constant tern to the input data\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    return model\n",
    "                   \n",
    "def model_metrics(X, y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    print('R2 Score: {:.3f}'.format(r2), '\\nMean Squared Error: {:.3f}'.format(mse), '\\nRoot MSE: {:.3f}'.format(rmse), '\\nMean Absolute Error: {:.3f}'.format(mae) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70b78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - Problem (case study)\n",
    "    # Data Description.\n",
    "    # Goal: use predictive analytics to analyze the most profitable customers and how to interact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79d6e3",
   "metadata": {},
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695f5841",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19492\\355808927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 02 - Getting Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLAB2_CSV_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19492\\112287885.py\u001b[0m in \u001b[0;36mimport_to_df\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4955\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4956\u001b[0m         \"\"\"\n\u001b[1;32m-> 4957\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4958\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4959\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4267\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4312\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6659\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6660\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6661\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6662\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6663\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# 02 - Getting Data\n",
    "\n",
    "df = import_to_df(LAB2_CSV_FILE)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce328a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Cleaning/Wrangling/EDA\n",
    "    # Change headers names.\n",
    "df = rename_columns(df, NEW_COLUMNS)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e102093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Cleaning/Wrangling/EDA:   \n",
    "    # Deal with NaN values:\n",
    "check_nan_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_nan_cols(df, PERCENTAGE_NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_nan_rows(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe21b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nan_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#03 - Cleaning/Wrangling/EDA:\n",
    "    # Split Categorical Numerical Features.\n",
    "# df['effective_to_date'] = pd.to_datetime(df['effective_to_date'], errors='coerce')\n",
    "df = to_object(df,'number_of_open_complaints', 'number_of_policies') \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c76779",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df, numerical_df = split_categorical_numerical(df)\n",
    "categorical_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2652d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#03 - Cleaning/Wrangling/EDA:\n",
    "    # Exploration\n",
    "categorical_df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b602f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba10a3",
   "metadata": {},
   "source": [
    "# Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data\n",
    "    # Dealing with outliers.\n",
    "    # Normalization.\n",
    "x, y = split_numerical(numerical_df, Y_VAR)\n",
    "y.shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data\n",
    "    # Use Log transformation to normalise data\n",
    "x_log = log_transformer(x, COLS_TO_TRANSFORM)\n",
    "plot_hist(x_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data\n",
    "    # Use Box-Cox Transformation to normalise data\n",
    "x_cox = box_cox_transformer(x, COLS_TO_TRANSFORM)\n",
    "plot_hist(x_cox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data\n",
    "    # Standardise data using Standard Scaler:\n",
    "x_cox_std = standard_scaler(x_cox)\n",
    "plot_hist(x_cox_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cox_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data\n",
    "    # Standardise data using Min-Max Scaler:\n",
    "x_cox_minmax = min_max_scaler(x_cox)\n",
    "plot_hist(x_cox_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cox_minmax.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cfec5",
   "metadata": {},
   "source": [
    "# Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data    \n",
    "    # Encoding Categorical Data: Label Encoding   \n",
    "categorical_df_le = label_encoder(categorical_df,LABEL_ENCODER_COLS)\n",
    "categorical_df_le.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Processing Data    \n",
    "    # Encoding Categorical Data: One Hot Encoder\n",
    "categorical_df_one = one_hot_encoder(categorical_df, ONE_HOT_COLS) \n",
    "categorical_df_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19316f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Concatenate Data Frames\n",
    "x_comb = concat_df(x_cox_std, categorical_df_le)\n",
    "x_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cox_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ae3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb[[15,16]] = x_comb[[15,16]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379836bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_comb, y, test_size=0.4, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5802e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "model_all = linear_regression(x_comb, y)\n",
    "model = linear_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f012c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(x_comb, y, model_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16defa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(X_test, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
